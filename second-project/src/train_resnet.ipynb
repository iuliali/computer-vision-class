{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Codul de antrenare pentru resnet task 1 si task 2 (kaggle)\n",
    "Predictiile se fac pe local, de aceea salvez state dict-ul modelului, nu modelul in sine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-20T10:36:16.558183Z",
     "iopub.status.busy": "2024-01-20T10:36:16.557797Z",
     "iopub.status.idle": "2024-01-20T10:36:16.566723Z",
     "shell.execute_reply": "2024-01-20T10:36:16.565863Z",
     "shell.execute_reply.started": "2024-01-20T10:36:16.558141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet18\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "import cv2 as cv\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "DIR_TRAIN = \"/kaggle/input/task-1-2-dataset\"\n",
    "DIR_CHARACTERS = \"characters\"\n",
    "POSITIVES = \"positives\"\n",
    "scaler = transforms.Resize((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "transform_flip_h = torchvision.transforms.RandomHorizontalFlip(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:36:18.460834Z",
     "iopub.status.busy": "2024-01-20T10:36:18.460124Z",
     "iopub.status.idle": "2024-01-20T10:36:18.466096Z",
     "shell.execute_reply": "2024-01-20T10:36:18.465141Z",
     "shell.execute_reply.started": "2024-01-20T10:36:18.460801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"barney\": 0,\n",
    "    \"betty\": 1,\n",
    "    \"fred\": 2,\n",
    "    \"wilma\": 3,\n",
    "    \"other\": 4\n",
    "}\n",
    "\n",
    "def getMapNameToLabel(image_name):\n",
    "    if image_name.split(\"_\")[0] in mapping:\n",
    "        return mapping[image_name.split(\"_\")[0]]\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:42:19.441999Z",
     "iopub.status.busy": "2024-01-20T10:42:19.441635Z",
     "iopub.status.idle": "2024-01-20T10:42:19.460693Z",
     "shell.execute_reply": "2024-01-20T10:42:19.459682Z",
     "shell.execute_reply.started": "2024-01-20T10:42:19.441968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class FFDataset(Dataset):\n",
    "    def __init__(self, character, transform=transform_flip_h):\n",
    "        self.characters = [\"fred\", \"barney\", \"wilma\", \"betty\"]\n",
    "        self.character = character\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        if character is not None: #task 2\n",
    "            print(\"task2\")\n",
    "            self.load_images(character=character, positives=True, task_1=False)\n",
    "            self.ex_positive= len(self.images)\n",
    "            print(\"gata exemplele pozitive: \", self.ex_positive)\n",
    "            self.ex_negative= 2 * self.ex_positive\n",
    "            self.load_images(character=character, positives=False, task_1=False)\n",
    "            print(\"gata exemplele negative: \", self.ex_negative)\n",
    "            print(\"all: \", len(self.images))\n",
    "        else: # task 1\n",
    "            self.load_images(task_1=True, positives=True)\n",
    "            self.ex_positive= len(self.images)\n",
    "            self.load_images(task_1=True, character=None)\n",
    "            self.ex_negatives = len(self.images) - self.ex_positive\n",
    "            \n",
    "\n",
    "    def load_images(self, task_1=False, positives=False, character=None):\n",
    "        path_dirs = []\n",
    "        if task_1 == False: # task 2\n",
    "            print(\"task 2\")\n",
    "            if positives and character is not None:\n",
    "                print(\"task2_ character\")\n",
    "                path_dirs = [os.path.join(DIR_TRAIN, DIR_CHARACTERS, character)]\n",
    "            else:\n",
    "                other_characters = [ch for ch in self.characters if ch != character]\n",
    "                path_dirs = [os.path.join(DIR_TRAIN, DIR_CHARACTERS, ch) for ch in other_characters]\n",
    "                path_dirs = path_dirs + [os.path.join(DIR_TRAIN, DIR_CHARACTERS, \"unknown\"),\n",
    "                             os.path.join(DIR_TRAIN, \"negatives\")]\n",
    "                examples = 0\n",
    "                print(examples)\n",
    "        else: # task 1\n",
    "            if positives:\n",
    "                path_dirs = [os.path.join(DIR_TRAIN, \"positives\")]\n",
    "            else:\n",
    "                path_dirs = [os.path.join(DIR_TRAIN, \"negatives\")]\n",
    "                examples = 0\n",
    "\n",
    "\n",
    "        for path_dir in path_dirs:\n",
    "            print(\"incarcam imag din : \", path_dir)\n",
    "            files = os.listdir(path_dir)\n",
    "            if positives==False and task_1==False:\n",
    "                files = random.choices(files, k=len(files) // 3)\n",
    "            for image_name in files:\n",
    "                image = cv.imread(os.path.join(path_dir, image_name))\n",
    "                img_test= Image.fromarray(image)\n",
    "                t_img = Variable(normalize(to_tensor(scaler(img_test))))\n",
    "                self.images.append(t_img)\n",
    "                \n",
    "                label = 0\n",
    "                if positives:\n",
    "                    label = 1\n",
    "                else:\n",
    "                    examples += 1\n",
    "                \n",
    "                self.labels.append(label)\n",
    "                \n",
    "                if (positives==False and task_1==False) and self.ex_negative <= examples:\n",
    "                    print(\"gata exemplele negative\")\n",
    "                    return\n",
    "                    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:33:00.195824Z",
     "iopub.status.busy": "2024-01-20T11:33:00.195456Z",
     "iopub.status.idle": "2024-01-20T11:33:00.370286Z",
     "shell.execute_reply": "2024-01-20T11:33:00.369522Z",
     "shell.execute_reply.started": "2024-01-20T11:33:00.195793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_EPOCHS = 50 # for task1 30, for task2 50\n",
    "\n",
    "my_model = resnet18(num_classes=2).to(DEVICE)\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(my_model.parameters(),momentum=0.9, lr=1e-3, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:33:02.445730Z",
     "iopub.status.busy": "2024-01-20T11:33:02.445337Z",
     "iopub.status.idle": "2024-01-20T11:33:23.927144Z",
     "shell.execute_reply": "2024-01-20T11:33:23.926229Z",
     "shell.execute_reply.started": "2024-01-20T11:33:02.445695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2\n",
      "task 2\n",
      "task2_ character\n",
      "incarcam imag din :  /kaggle/input/task-1-2-dataset/characters/wilma\n",
      "gata exemplele pozitive:  1605\n",
      "task 2\n",
      "0\n",
      "incarcam imag din :  /kaggle/input/task-1-2-dataset/characters/fred\n",
      "incarcam imag din :  /kaggle/input/task-1-2-dataset/characters/barney\n",
      "incarcam imag din :  /kaggle/input/task-1-2-dataset/characters/betty\n",
      "incarcam imag din :  /kaggle/input/task-1-2-dataset/characters/unknown\n",
      "incarcam imag din :  /kaggle/input/task-1-2-dataset/negatives\n",
      "gata exemplele negative\n",
      "gata exemplele negative:  3210\n",
      "all:  4815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "dataset = FFDataset('wilma') # task 2, pt task 1 nu se pune nimic <-> dataset = FFDataset(None)\n",
    "train_data = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = .20\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                 sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:33:58.678674Z",
     "iopub.status.busy": "2024-01-20T11:33:58.677925Z",
     "iopub.status.idle": "2024-01-20T11:43:29.716507Z",
     "shell.execute_reply": "2024-01-20T11:43:29.715482Z",
     "shell.execute_reply.started": "2024-01-20T11:33:58.678633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Epoch 1--\n",
      "Batch index 0; learning rate 0.001; loss: 0.735953\n",
      "Batch index 50; learning rate 0.001; loss: 0.169489\n",
      "Batch index 100; learning rate 0.001; loss: 0.094984\n",
      "mean loss: 0.28507646918296814\n",
      "--Epoch 2--\n",
      "Batch index 0; learning rate 0.001; loss: 0.058144\n",
      "Batch index 50; learning rate 0.001; loss: 0.251812\n",
      "Batch index 100; learning rate 0.001; loss: 0.034581\n",
      "mean loss: 0.0942503958940506\n",
      "--Epoch 3--\n",
      "Batch index 0; learning rate 0.001; loss: 0.188081\n",
      "Batch index 50; learning rate 0.001; loss: 0.009487\n",
      "Batch index 100; learning rate 0.001; loss: 0.012361\n",
      "mean loss: 0.08667925000190735\n",
      "--Epoch 4--\n",
      "Batch index 0; learning rate 0.001; loss: 0.079215\n",
      "Batch index 50; learning rate 0.001; loss: 0.041887\n",
      "Batch index 100; learning rate 0.001; loss: 0.105426\n",
      "mean loss: 0.0726318508386612\n",
      "--Epoch 5--\n",
      "Batch index 0; learning rate 0.001; loss: 0.108980\n",
      "Batch index 50; learning rate 0.001; loss: 0.025751\n",
      "Batch index 100; learning rate 0.001; loss: 0.003034\n",
      "mean loss: 0.057843852788209915\n",
      "--Epoch 6--\n",
      "Batch index 0; learning rate 0.001; loss: 0.151331\n",
      "Batch index 50; learning rate 0.001; loss: 0.036109\n",
      "Batch index 100; learning rate 0.001; loss: 0.022572\n",
      "mean loss: 0.054763808846473694\n",
      "--Epoch 7--\n",
      "Batch index 0; learning rate 0.001; loss: 0.012722\n",
      "Batch index 50; learning rate 0.001; loss: 0.092606\n",
      "Batch index 100; learning rate 0.001; loss: 0.045331\n",
      "mean loss: 0.04796477034687996\n",
      "--Epoch 8--\n",
      "Batch index 0; learning rate 0.001; loss: 0.060874\n",
      "Batch index 50; learning rate 0.001; loss: 0.052183\n",
      "Batch index 100; learning rate 0.001; loss: 0.065437\n",
      "mean loss: 0.03890218958258629\n",
      "--Epoch 9--\n",
      "Batch index 0; learning rate 0.001; loss: 0.079239\n",
      "Batch index 50; learning rate 0.001; loss: 0.034650\n",
      "Batch index 100; learning rate 0.001; loss: 0.078353\n",
      "mean loss: 0.046072375029325485\n",
      "--Epoch 10--\n",
      "Batch index 0; learning rate 0.001; loss: 0.005872\n",
      "Batch index 50; learning rate 0.001; loss: 0.037993\n",
      "Batch index 100; learning rate 0.001; loss: 0.052622\n",
      "mean loss: 0.03641754761338234\n",
      "--Epoch 11--\n",
      "Batch index 0; learning rate 0.001; loss: 0.006511\n",
      "Batch index 50; learning rate 0.001; loss: 0.009397\n",
      "Batch index 100; learning rate 0.001; loss: 0.003543\n",
      "mean loss: 0.0348631925880909\n",
      "--Epoch 12--\n",
      "Batch index 0; learning rate 0.001; loss: 0.027187\n",
      "Batch index 50; learning rate 0.001; loss: 0.010861\n",
      "Batch index 100; learning rate 0.001; loss: 0.014699\n",
      "mean loss: 0.030723027884960175\n",
      "--Epoch 13--\n",
      "Batch index 0; learning rate 0.001; loss: 0.002583\n",
      "Batch index 50; learning rate 0.001; loss: 0.069631\n",
      "Batch index 100; learning rate 0.001; loss: 0.003861\n",
      "mean loss: 0.033884353935718536\n",
      "--Epoch 14--\n",
      "Batch index 0; learning rate 0.001; loss: 0.002585\n",
      "Batch index 50; learning rate 0.001; loss: 0.066621\n",
      "Batch index 100; learning rate 0.001; loss: 0.001608\n",
      "mean loss: 0.02992042899131775\n",
      "--Epoch 15--\n",
      "Batch index 0; learning rate 0.001; loss: 0.008636\n",
      "Batch index 50; learning rate 0.001; loss: 0.147852\n",
      "Batch index 100; learning rate 0.001; loss: 0.004156\n",
      "mean loss: 0.020378483459353447\n",
      "--Epoch 16--\n",
      "Batch index 0; learning rate 0.001; loss: 0.006235\n",
      "Batch index 50; learning rate 0.001; loss: 0.011861\n",
      "Batch index 100; learning rate 0.001; loss: 0.011342\n",
      "mean loss: 0.03489679470658302\n",
      "--Epoch 17--\n",
      "Batch index 0; learning rate 0.001; loss: 0.019131\n",
      "Batch index 50; learning rate 0.001; loss: 0.007187\n",
      "Batch index 100; learning rate 0.001; loss: 0.001545\n",
      "mean loss: 0.037099603563547134\n",
      "--Epoch 18--\n",
      "Batch index 0; learning rate 0.001; loss: 0.038105\n",
      "Batch index 50; learning rate 0.001; loss: 0.000965\n",
      "Batch index 100; learning rate 0.001; loss: 0.032513\n",
      "mean loss: 0.03117731213569641\n",
      "--Epoch 19--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001128\n",
      "Batch index 50; learning rate 0.001; loss: 0.002999\n",
      "Batch index 100; learning rate 0.001; loss: 0.002286\n",
      "mean loss: 0.0175529383122921\n",
      "--Epoch 20--\n",
      "Batch index 0; learning rate 0.001; loss: 0.013187\n",
      "Batch index 50; learning rate 0.001; loss: 0.002517\n",
      "Batch index 100; learning rate 0.001; loss: 0.006053\n",
      "mean loss: 0.018135802820324898\n",
      "--Epoch 21--\n",
      "Batch index 0; learning rate 0.001; loss: 0.006625\n",
      "Batch index 50; learning rate 0.001; loss: 0.017120\n",
      "Batch index 100; learning rate 0.001; loss: 0.002142\n",
      "mean loss: 0.01632639765739441\n",
      "--Epoch 22--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001114\n",
      "Batch index 50; learning rate 0.001; loss: 0.017269\n",
      "Batch index 100; learning rate 0.001; loss: 0.004253\n",
      "mean loss: 0.017346156761050224\n",
      "--Epoch 23--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001727\n",
      "Batch index 50; learning rate 0.001; loss: 0.012563\n",
      "Batch index 100; learning rate 0.001; loss: 0.000773\n",
      "mean loss: 0.019455572590231895\n",
      "--Epoch 24--\n",
      "Batch index 0; learning rate 0.001; loss: 0.004168\n",
      "Batch index 50; learning rate 0.001; loss: 0.003156\n",
      "Batch index 100; learning rate 0.001; loss: 0.001073\n",
      "mean loss: 0.01394178532063961\n",
      "--Epoch 25--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000722\n",
      "Batch index 50; learning rate 0.001; loss: 0.002230\n",
      "Batch index 100; learning rate 0.001; loss: 0.000517\n",
      "mean loss: 0.007485688664019108\n",
      "--Epoch 26--\n",
      "Batch index 0; learning rate 0.001; loss: 0.008395\n",
      "Batch index 50; learning rate 0.001; loss: 0.010999\n",
      "Batch index 100; learning rate 0.001; loss: 0.044720\n",
      "mean loss: 0.013243371620774269\n",
      "--Epoch 27--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000996\n",
      "Batch index 50; learning rate 0.001; loss: 0.002428\n",
      "Batch index 100; learning rate 0.001; loss: 0.026393\n",
      "mean loss: 0.008863815106451511\n",
      "--Epoch 28--\n",
      "Batch index 0; learning rate 0.001; loss: 0.004494\n",
      "Batch index 50; learning rate 0.001; loss: 0.006456\n",
      "Batch index 100; learning rate 0.001; loss: 0.001392\n",
      "mean loss: 0.005572563968598843\n",
      "--Epoch 29--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001295\n",
      "Batch index 50; learning rate 0.001; loss: 0.003250\n",
      "Batch index 100; learning rate 0.001; loss: 0.003637\n",
      "mean loss: 0.007417530287057161\n",
      "--Epoch 30--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000699\n",
      "Batch index 50; learning rate 0.001; loss: 0.002186\n",
      "Batch index 100; learning rate 0.001; loss: 0.000403\n",
      "mean loss: 0.005596564617007971\n",
      "--Epoch 31--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000286\n",
      "Batch index 50; learning rate 0.001; loss: 0.002118\n",
      "Batch index 100; learning rate 0.001; loss: 0.001588\n",
      "mean loss: 0.012022552080452442\n",
      "--Epoch 32--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001122\n",
      "Batch index 50; learning rate 0.001; loss: 0.002488\n",
      "Batch index 100; learning rate 0.001; loss: 0.001245\n",
      "mean loss: 0.012936675921082497\n",
      "--Epoch 33--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001330\n",
      "Batch index 50; learning rate 0.001; loss: 0.001229\n",
      "Batch index 100; learning rate 0.001; loss: 0.000926\n",
      "mean loss: 0.006862349342554808\n",
      "--Epoch 34--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000297\n",
      "Batch index 50; learning rate 0.001; loss: 0.003738\n",
      "Batch index 100; learning rate 0.001; loss: 0.000234\n",
      "mean loss: 0.011731338687241077\n",
      "--Epoch 35--\n",
      "Batch index 0; learning rate 0.001; loss: 0.005054\n",
      "Batch index 50; learning rate 0.001; loss: 0.000934\n",
      "Batch index 100; learning rate 0.001; loss: 0.006939\n",
      "mean loss: 0.007696524728089571\n",
      "--Epoch 36--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000965\n",
      "Batch index 50; learning rate 0.001; loss: 0.001902\n",
      "Batch index 100; learning rate 0.001; loss: 0.006722\n",
      "mean loss: 0.00573677197098732\n",
      "--Epoch 37--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001030\n",
      "Batch index 50; learning rate 0.001; loss: 0.001904\n",
      "Batch index 100; learning rate 0.001; loss: 0.004175\n",
      "mean loss: 0.0031818386632949114\n",
      "--Epoch 38--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000251\n",
      "Batch index 50; learning rate 0.001; loss: 0.015187\n",
      "Batch index 100; learning rate 0.001; loss: 0.000856\n",
      "mean loss: 0.005119821522384882\n",
      "--Epoch 39--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000873\n",
      "Batch index 50; learning rate 0.001; loss: 0.012144\n",
      "Batch index 100; learning rate 0.001; loss: 0.011970\n",
      "mean loss: 0.005921137519180775\n",
      "--Epoch 40--\n",
      "Batch index 0; learning rate 0.001; loss: 0.004349\n",
      "Batch index 50; learning rate 0.001; loss: 0.002182\n",
      "Batch index 100; learning rate 0.001; loss: 0.001193\n",
      "mean loss: 0.0022142387460917234\n",
      "--Epoch 41--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000376\n",
      "Batch index 50; learning rate 0.001; loss: 0.076418\n",
      "Batch index 100; learning rate 0.001; loss: 0.000962\n",
      "mean loss: 0.006623894441872835\n",
      "--Epoch 42--\n",
      "Batch index 0; learning rate 0.001; loss: 0.005977\n",
      "Batch index 50; learning rate 0.001; loss: 0.000133\n",
      "Batch index 100; learning rate 0.001; loss: 0.042339\n",
      "mean loss: 0.0017898455262184143\n",
      "--Epoch 43--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000395\n",
      "Batch index 50; learning rate 0.001; loss: 0.002844\n",
      "Batch index 100; learning rate 0.001; loss: 0.000825\n",
      "mean loss: 0.0030780709348618984\n",
      "--Epoch 44--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000244\n",
      "Batch index 50; learning rate 0.001; loss: 0.000104\n",
      "Batch index 100; learning rate 0.001; loss: 0.030227\n",
      "mean loss: 0.0028006872162222862\n",
      "--Epoch 45--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001070\n",
      "Batch index 50; learning rate 0.001; loss: 0.000261\n",
      "Batch index 100; learning rate 0.001; loss: 0.000305\n",
      "mean loss: 0.001776231685653329\n",
      "--Epoch 46--\n",
      "Batch index 0; learning rate 0.001; loss: 0.001297\n",
      "Batch index 50; learning rate 0.001; loss: 0.001938\n",
      "Batch index 100; learning rate 0.001; loss: 0.000555\n",
      "mean loss: 0.004253060091286898\n",
      "--Epoch 47--\n",
      "Batch index 0; learning rate 0.001; loss: 0.004231\n",
      "Batch index 50; learning rate 0.001; loss: 0.001295\n",
      "Batch index 100; learning rate 0.001; loss: 0.000859\n",
      "mean loss: 0.007646801881492138\n",
      "--Epoch 48--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000283\n",
      "Batch index 50; learning rate 0.001; loss: 0.000429\n",
      "Batch index 100; learning rate 0.001; loss: 0.005460\n",
      "mean loss: 0.0032395769376307726\n",
      "--Epoch 49--\n",
      "Batch index 0; learning rate 0.001; loss: 0.002417\n",
      "Batch index 50; learning rate 0.001; loss: 0.054035\n",
      "Batch index 100; learning rate 0.001; loss: 0.006442\n",
      "mean loss: 0.0067307427525520325\n",
      "--Epoch 50--\n",
      "Batch index 0; learning rate 0.001; loss: 0.000251\n",
      "Batch index 50; learning rate 0.001; loss: 0.002436\n",
      "Batch index 100; learning rate 0.001; loss: 0.000119\n",
      "mean loss: 0.003971201833337545\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "my_model.train(True)\n",
    "loss_mean=0\n",
    "losses=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"--Epoch {epoch+1}--\")\n",
    "    losses = 0\n",
    "    loss_mean=0\n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        out = my_model(images)\n",
    "        loss = cross_entropy_loss(out, labels)\n",
    "        loss_mean += loss\n",
    "        losses += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(f\"Batch index {batch}; learning rate {optimizer.param_groups[0]['lr']}; loss: {loss.item():>8f}\")\n",
    "            \n",
    "    print(f\"mean loss: {loss_mean/losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:45:39.997044Z",
     "iopub.status.busy": "2024-01-20T11:45:39.996664Z",
     "iopub.status.idle": "2024-01-20T11:45:41.040952Z",
     "shell.execute_reply": "2024-01-20T11:45:41.040029Z",
     "shell.execute_reply.started": "2024-01-20T11:45:39.997011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.3%, Loss: 0.000873 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#validare\n",
    "correct = 0\n",
    "test_loss = 0\n",
    "size = len(validation_loader.dataset) * validation_split\n",
    "val_labels = []\n",
    "predicted_val = []\n",
    "my_model.to(DEVICE)\n",
    "my_model.eval()\n",
    "with torch.no_grad():\n",
    "    for image_batch, labels_batch in validation_loader:\n",
    "        image_batch = image_batch.to(DEVICE)\n",
    "        labels_batch = labels_batch.to(DEVICE)\n",
    "        pred = my_model(image_batch) \n",
    "        test_loss += cross_entropy_loss(pred, labels_batch).item()\n",
    "        correct += (pred.argmax(1) == labels_batch).type(torch.float).sum().item()\n",
    "        predicted_val.extend(pred.argmax(1))\n",
    "        val_labels.extend(labels_batch)\n",
    "\n",
    "correct /= size\n",
    "test_loss /= size\n",
    "print(f\"Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:46:05.025204Z",
     "iopub.status.busy": "2024-01-20T11:46:05.024830Z",
     "iopub.status.idle": "2024-01-20T11:46:05.100034Z",
     "shell.execute_reply": "2024-01-20T11:46:05.099162Z",
     "shell.execute_reply.started": "2024-01-20T11:46:05.025173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wd = \"/kaggle/working/\"\n",
    "torch.save(my_model.state_dict(), wd + \"model_dict_3_2_1\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4292802,
     "sourceId": 7385595,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
